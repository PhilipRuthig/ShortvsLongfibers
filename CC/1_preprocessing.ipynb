{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMTOOLS -- Preprocessing\n",
    "This notebook preprocesses images for prediction using a DNN trained with Uni-EM. It opens a series of .tiff or .png files in `path_input` and applies CLAHE (Contrast limited adaptive histogram equalization), which enhances the local contrast of images. It then re-saves them as RGB .png files to `path_results`.\n",
    "\n",
    "**Author:** Philip Ruthig, Paul Flechsig Institute, Center of Neuropathology and Brain Research Leipzig\n",
    "\n",
    "**Contact:** philip.ruthig@medizin.uni-leipzig.de // philip.ruthig@gmail.com\n",
    "\n",
    "**Publication:**\n",
    "Please contact me if you want to use this code for any publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hoellenmaschine2\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import tifffile as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import skimage\n",
    "import scipy.stats as stats\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(image_array, batch_size, overlap):\n",
    "    img_list = []\n",
    "    yx_list = []\n",
    "\n",
    "    for i in range(0, image_array.shape[0] - batch_size + 1, batch_size - overlap):\n",
    "        for j in range(0, image_array.shape[1] - batch_size + 1, batch_size - overlap):\n",
    "            if i + batch_size > image_array.shape[0]:\n",
    "                i = image_array.shape[0] - batch_size\n",
    "            if j + batch_size > image_array.shape[1]:\n",
    "                j = image_array.shape[1] - batch_size\n",
    "\n",
    "            batch = image_array[i:i+batch_size, j:j+batch_size]\n",
    "            img_list.append(batch)\n",
    "            yx_list.append((i,i+batch_size,j,j+batch_size))\n",
    "\n",
    "    return img_list, yx_list\n",
    "\n",
    "def pad_image(image_array, batch_size, overlap):\n",
    "    original_shape = image_array.shape\n",
    "\n",
    "    # Calculate the required padding to achieve the desired size\n",
    "    pad_rows = max(0, batch_size - overlap - (original_shape[0] % (batch_size - overlap)))\n",
    "    pad_cols = max(0, batch_size - overlap - (original_shape[1] % (batch_size - overlap)))\n",
    "\n",
    "    # Calculate the total size based on the desired overlap\n",
    "    total_rows = original_shape[0] + pad_rows\n",
    "    total_cols = original_shape[1] + pad_cols\n",
    "\n",
    "    # Calculate the excess padding beyond the desired size\n",
    "    excess_rows = total_rows % batch_size\n",
    "    excess_cols = total_cols % batch_size\n",
    "\n",
    "    # Adjust the total size to the desired size, accounting for the excess padding\n",
    "    total_rows -= excess_rows\n",
    "    total_cols -= excess_cols\n",
    "\n",
    "    padded_image = np.pad(image_array, ((0, total_rows), (0, total_cols)), mode='reflect')\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def remove_whitespaces(string):\n",
    "    return \"\".join(string.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "path_input = r\"0_raw\\\\\"\n",
    "path_results = r\"1_preprocessed\\\\\"\n",
    "ds = 4 # each axis of the image is downsampled by this factor.\n",
    "batch_size = 2048 # width and length of each image.\n",
    "overlap = 200 # overlap of each neighbouring image batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_list = []\n",
    "img_coord_list = []\n",
    "img_original_shape_list = []\n",
    "img_original_name_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "path_images = [f for f in listdir(path_input) if isfile(join(path_input, f)) and f != \".gitkeep\"]\n",
    "for i1 in tqdm.tqdm(range(len(path_images)),position=0,leave=False):\n",
    "    try:\n",
    "        if path_images[i1].endswith(\".tif\") or path_images[i1].endswith(\".tiff\") or path_images[i1].endswith(\".TIF\") or path_images[i1].endswith(\".TIFF\"):\n",
    "            test_img = tf.imread(path_input + str(path_images[i1])) # use this for tiff\n",
    "        elif path_images[i1].endswith(\".png\") or path_images[i1].endswith(\".PNG\"):\n",
    "            test_img = cv2.imread(path_input + str(path_images[i1]),-1) # use this for png \n",
    "            temp = (test_img[:,:,0]+test_img[:,:,1]+test_img[:,:,2])/3 # average RGB to grayscale\n",
    "            test_img = temp.astype('uint8')\n",
    "        else:\n",
    "            print('Input file format not supported. Use .png or .tif.')\n",
    "            break\n",
    "        test_img_ds = downscale_local_mean(test_img, ds)\n",
    "        test_img_ds_pad = pad_image(test_img_ds, batch_size, overlap)\n",
    "        img_list,coords = batch_generator(test_img_ds_pad, batch_size, overlap)\n",
    "        for i2 in tqdm.tqdm(range(len(img_list)),position=1,leave=False):\n",
    "            test_img_clahe = skimage.exposure.equalize_adapthist(img_list[i2]/np.max(img_list[i2]),clip_limit=0.01,)#kernel_size=127)\n",
    "            test_img_rgb_png = cv2.merge((downscale_local_mean(test_img_clahe,1),#R\n",
    "                                        downscale_local_mean(test_img_clahe,1),  #G\n",
    "                                        downscale_local_mean(test_img_clahe,1))) #B\n",
    "            skimage.io.imsave(path_results + path_images[i1][:-4] + remove_whitespaces(str(coords[i2])) +  \".png\", (test_img_rgb_png*255).astype('uint8'))\n",
    "            img_name_list.append(path_images[i1][:-4] + remove_whitespaces(str(coords[i2])) +  \".png\")\n",
    "            img_coord_list.append(coords[i2])\n",
    "            img_original_name_list.append(path_images[i1][:-4])\n",
    "        img_original_shape_list.append(test_img_ds.shape)\n",
    "    except:\n",
    "        print(f\"Failed for Image: {path_images[i1]}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata as pkl files\n",
    "with open(r\"img_name_list\",\"wb\") as fp:\n",
    "    pickle.dump(img_name_list,fp)\n",
    "\n",
    "with open(r\"img_coord_list\",\"wb\") as fp:\n",
    "    pickle.dump(img_coord_list,fp)\n",
    "\n",
    "with open(r\"img_original_shape_list\",\"wb\") as fp:\n",
    "    pickle.dump(img_original_shape_list,fp)\n",
    "\n",
    "with open(r\"img_original_name_list\",\"wb\") as fp:\n",
    "    pickle.dump(img_original_name_list,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_background(img, background_threshold):\n",
    "#     '''\n",
    "#     Returns the largest rectangular region within your 2d image that does not contain background.\n",
    "#     Useful if your images contain grid shadows or stitching artifacts and you want to exclude them from your analysis.\n",
    "#     '''\n",
    "#     rows, cols = img.shape\n",
    "#     max_area = 0\n",
    "#     max_top = max_left = max_bottom = max_right = 0\n",
    "\n",
    "#     # Loop through each element in the 2D array (image)\n",
    "#     for i in range(rows):\n",
    "#         for j in range(cols):\n",
    "#             # Check if the element meets the background threshold condition\n",
    "#             if img[i, j] >= background_threshold:\n",
    "#                 top = bottom = i\n",
    "#                 left = right = j\n",
    "\n",
    "#                 # Expand the region vertically until the background threshold condition is not met\n",
    "#                 while bottom < rows and img[bottom, j] >= background_threshold:\n",
    "#                     bottom += 1\n",
    "\n",
    "#                 # Expand the region horizontally until the background threshold condition is not met\n",
    "#                 while right < cols and np.all(img[i:bottom, right] >= background_threshold):\n",
    "#                     right += 1\n",
    "\n",
    "#                 # Calculate the area of the current rectangular region\n",
    "#                 area = (bottom - i) * (right - j)\n",
    "\n",
    "#                 # Update the maximum area and the coordinates of the maximum rectangular region\n",
    "#                 if area > max_area:\n",
    "#                     max_area = area\n",
    "#                     max_top, max_left, max_bottom, max_right = i, j, bottom, right\n",
    "\n",
    "#     # Return the cropped region\n",
    "#     return img[max_top:max_bottom, max_left:max_right]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
